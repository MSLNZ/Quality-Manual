\section{Data management}
\begin{flushright}
\textit{No problem is too small or too trivial if we can really do something about it.} \\
-- Richard Feynman 
\end{flushright}


Digital data and associated software are critical MSL assets. Like any other assets, they should be cared for and maintained. This section is about data management planning. 

\subsection{Data management planning}
A Data Management Plan (DMP) describes how data is managed, from initial production, through data processing to, finally, long-term storage; it should cover how data is handled during and after the completion of a project and who is responsible.

The DMP should contain sufficient detail to enable stakeholders (MSL staff, managers, and IT support) to understand the requirements for data management.

\subsubsection{Points to be considered in a data management plan}
\begin{enumerate}
 \item How is data generated? What software is used? 
 \item Data formats (e.g., raw text, .csv, .xlsx, XML, etc). 
 \item Is data annotated (with metadata)? Are standards used (which)? 
 \item Where is data stored? How is it organised (common file structure, database, etc)? 
 \item Size of data (storage and networking requirements)? 
 \item How is initial (raw) data secured? Level of protection required? 
 \item How is data processed? What software is used? 
 \item How is data integrity maintained? Is there an audit trail?
 \item Do policies limit access to, and sharing of, (raw or processed) data? How are these policies implemented?
 \item How are final (published) reports and data related (records of processing)?
 \item How are raw data, processed data, and reporting outputs archived? 
 \item How is software maintained over time?
 \item How long must data and software be retained?  
 \item Responsibilities for data (curation, stewardship, custody)?\footnote{A `custodian`  is usually a manger with responsibility for the business function of data; a `steward` is a subject-matter expert with responsibility for how data is used. Curation manages data through its life-cycle of usefulness to science and education. A curator may organise data integration from various sources, which may be needed as part of research projects. }
\end{enumerate}

\subsubsection{Example}
\paragraph{Raw data} is generated by bespoke data acquisition software written in Python. This software is developed under version control (git). A github repository is used, which is cloned on lab and office machines. A record of the version(s) used in a job is logged when raw data is collected.  

The main data formats are: plain text files, spreadsheet files (.xls and.xlsx) and Python source (text files with extension .py). The text and Python files are largely used for configuration and metadata; the spreadsheet files contain measurement data. Results of data processing are also generated in plain text and in spreadsheet files; there is also one proprietary binary file format for some data. 

Note, spreadsheets containing data may exploited for \textit{ad hoc} calculations but the main data analysis is done in Python. There are no spreadsheet software requirements.

No metadata standards are followed. 

\paragraph{Data is organised} in a hierarchy of sub-folders under a job-specific root folder; sub-folder names are generally composed of a time-stamp added to a descriptive root (e.g., Fig.~\ref{fig:runs} shows sub-folders where the `sp' stands for S-parameters). The results of data processing are stored in a similar way, using folder names that begin
with `dp' and have a date and time stamp (see Fig.~\ref{fig:folders}) The storage requirements of a completed job is typically several megabytes.

\begin{figure}[ht]
 \centering
  \includegraphics[width=0.8\linewidth]{pictures/filesystem_vna_runs.png}
  \caption{Measurement runs (sub-folders) inside an `sp' measurement folder (pane on the left) and the contents of one run folder (pane on the right).}
  \label{fig:runs}
\end{figure}

\begin{figure}[ht]
 \centering
  \includegraphics[width=0.8\linewidth]{pictures/filesystem_runs.png}
  \caption{View of a root folder structure for one job (the root is `first', organised below `two\_port', the year, and `50dB\_check'), containing three data processing sub-folders as well as VNA sub-folders in which raw data and configuration information is stored.}
  \label{fig:folders}
\end{figure}

\paragraph{Data integrity} is maintained by a git repository for each job. The initial job set-up creates a bare repository on the corporate file server (I-drive), which is immediately cloned to the lab computer. After raw data has been collected, it is committed to the local repository by the metrologist and then a copy is pushed to the I-drive. 

This produces a (complete and independent) lab copy of the data and a remote copy on the I-drive; the I-drive is also routinely backed up (24-hour basis). (This workflow also guards against intermittent network problems by allowing work to be carried out locally until network access is restored.)

Using a repository guards against unintended accidental changes to data.

\paragraph{All data} is confidential to MSL by default. So, the `private' I-drive allocated to MSL is used, which restricts access to MSL staff. No other privacy protections are necessary.  

\paragraph{Data is processed} by bespoke Python software (also version-controlled) and the results stored in the same folder structure (see Fig.~\ref{fig:folders}). This allows the git repository to capture all stages of analysis, facilitating revision of processing details at a later date, or to carry out independent processing on the same data without loss of information obtained earlier. 

When final client-specific analysis steps are required, the Python script for these last steps will be stored in the section file system commercial job file, which is separately located on the I-drive. The final report is also saved in this commercial job file. 

\paragraph{On completion} of a job, the repository remains undisturbed on local computers and the I-drive. There is no specific `archiving' process. The amount of data does not warrant compression and any eventual future analysis will be facilitated by maintaining the local file structure. 

Data and software must be retained indefinitely. 

The reliance on open-source Python software will help to provide on-going access to the functionality of our software. Python, of course, evolves and regular upgrades are expected. A suite of unit-test cases are used to facilitate migration and acceptance-testing of new versions.

\paragraph{Responsibilities} for the data relate to custody and stewardship; curation is not required. The MSL Director is the custodian of data, but delegates this to the Team Manager in charge of the section. The data stewards are qualified MSL staff (MSL Competency Matrix) for the measurement procedure. 
